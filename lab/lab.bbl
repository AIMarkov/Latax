\begin{thebibliography}{10}

\bibitem{Rummery1}
G~A.~Rummery and Mahesan Niranjan.
\newblock On-line q-learning using connectionist systems.
\newblock 11 1994.

\bibitem{gym1}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em CoRR}, abs/1606.01540, 2016.

\bibitem{Ciosek1}
Kamil Ciosek and Shimon Whiteson.
\newblock Expected policy gradients.
\newblock In {\em Proceedings of the Thirty-Second {AAAI} Conference on
  Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018},
  2018.

\bibitem{fent1}
Jun Feng, Minlie Huang, Li~Zhao, Yang Yang, and Xiaoyan Zhu.
\newblock Reinforcement learning for relation classification from noisy data.
\newblock In {\em Proceedings of the Thirty-Second {AAAI} Conference on
  Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018},
  2018.

\bibitem{Foerster1}
Jakob~N Foerster, Yannis~M Assael, Nando De~Freitas, and Shimon Whiteson.
\newblock Learning to communicate with deep multi−agent reinforcement
  learning.
\newblock pages 2137--2145, Barcelona,SPAIN, 2016.

\bibitem{ganin1}
Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S.~M.~Ali Eslami, and Oriol
  Vinyals.
\newblock Synthesizing programs for images using reinforced adversarial
  learning.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 1652--1661, 2018.

\bibitem{Gao1}
Y.~Gao, R.~Y. Zhou, H.~Wang, and Z.~X. Cao.
\newblock Study on an average reward reinforcement learning algorithm.
\newblock {\em Chinese Journal of Computers}, 30(8):1372--1378, 2007.

\bibitem{gu1}
Shixiang Gu, Timothy~P. Lillicrap, Ilya Sutskever, and Sergey Levine.
\newblock Continuous deep q-learning with model-based acceleration.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016}, pages
  2829--2838, 2016.

\bibitem{Heess1}
Nicolas Heess, Jonathan~J. Hunt, Timothy~P. Lillicrap, and David Silver.
\newblock Memory-based control with recurrent neural networks.
\newblock {\em CoRR}, abs/1512.04455, 2015.

\bibitem{Hester1}
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal
  Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, Gabriel
  Dulac{-}Arnold, John Agapiou, Joel~Z. Leibo, and Audrunas Gruslys.
\newblock Deep q-learning from demonstrations.
\newblock In {\em Proceedings of the Thirty-Second {AAAI} Conference on
  Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018},
  2018.

\bibitem{Konda1}
Vijaymohan Konda.
\newblock {\em Actor-critic algorithms}.
\newblock PhD thesis, Massachusetts Institute of Technology, Cambridge, MA,
  {USA}, 2002.

\bibitem{lange1}
Sascha Lange, Martin~A. Riedmiller, and Arne Voigtl{\"{a}}nder.
\newblock Autonomous reinforcement learning on raw visual input data in a real
  world application.
\newblock In {\em The 2012 International Joint Conference on Neural Networks
  (IJCNN), Brisbane, Australia, June 10-15, 2012}, pages 1--8, 2012.

\bibitem{leibo1}
Joel~Z. Leibo, Vin{\'{\i}}cius~Flores Zambaldi, Marc Lanctot, Janusz Marecki,
  and Thore Graepel.
\newblock Multi-agent reinforcement learning in sequential social dilemmas.
\newblock In {\em Proceedings of the 16th Conference on Autonomous Agents and
  MultiAgent Systems, {AAMAS} 2017, S{\~{a}}o Paulo, Brazil, May 8-12, 2017},
  pages 464--473, 2017.

\bibitem{li1}
Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng
  Gao.
\newblock Deep reinforcement learning for dialogue generation.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing, {EMNLP} 2016, Austin, Texas, USA, November 1-4,
  2016}, pages 1192--1202, 2016.

\bibitem{lillicraps1}
Timothy~P. Lillicrap, Jonathan~J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em CoRR}, abs/1509.02971, 2015.

\bibitem{lin2}
Long~Ji Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock {\em Machine Learning}, 8:293--321, 1992.

\bibitem{lowe1}
Ryan Lowe, Yi~Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, 4-9 December 2017,
  Long Beach, CA, {USA}}, pages 6382--6393, 2017.

\bibitem{mnih2}
Volodymyr Mnih, Adri{\`{a}}~Puigdom{\`{e}}nech Badia, Mehdi Mirza, Alex Graves,
  Timothy~P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016}, pages
  1928--1937, 2016.

\bibitem{mnih1}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin~A Riedmiller, Andreas~K Fidjeland,
  Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533, 2015.

\bibitem{Morimura1}
Tetsuro Morimura, Eiji Uchibe, Junichiro Yoshimoto, and Kenji Doya.
\newblock A generalized natural actor-critic algorithm.
\newblock In {\em Advances in Neural Information Processing Systems 22: 23rd
  Annual Conference on Neural Information Processing Systems 2009. Proceedings
  of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada.},
  pages 1312--1320, 2009.

\bibitem{peters1}
Jan Peters and Stefan Schaal.
\newblock Natural actor-critic.
\newblock {\em Neurocomputing}, 71(7-9):1180--1190, 2008.

\bibitem{silver2}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre,
  George~Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em Nature}, 529(7587):484--489, 2016.

\bibitem{silver1}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin~A. Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In {\em Proceedings of the 31th International Conference on Machine
  Learning, {ICML} 2014, Beijing, China, 21-26 June 2014}, pages 387--395,
  2014.

\bibitem{Singh1}
Satinder~P. Singh and Richard~S. Sutton.
\newblock Reinforcement learning with replacing eligibility traces.
\newblock {\em Machine Learning}, 22(1-3):123--158, Mar 1996.

\bibitem{sutton1998}
Richard S.Sutton and Andrew G.Barto.
\newblock {\em Reinforcement learning:an introduction}.
\newblock The MIT press, 1998.

\bibitem{sutton2017}
Richard S.Sutton and Andrew G.Barto.
\newblock {\em Reinforcement learning：an introduction(Complete Draft)}.
\newblock The MIT press, 2017.

\bibitem{SuttonA1}
Richard~S. Sutton, David~A. McAllester, Satinder~P. Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in Neural Information Processing Systems 12, {[NIPS}
  Conference, Denver, Colorado, USA, November 29 - December 4, 1999]}, pages
  1057--1063, 1999.

\bibitem{Tangkaratt1}
Voot Tangkaratt, Abbas Abdolmaleki, and Masashi Sugiyama.
\newblock Guide actor-critic for continuous control.
\newblock Vancouver Convention Center, Vancouver CANADA, 2018.

\bibitem{Mujoco1}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: {A} physics engine for model-based control.
\newblock In {\em 2012 {IEEE/RSJ} International Conference on Intelligent
  Robots and Systems, {IROS} 2012, Vilamoura, Algarve, Portugal, October 7-12,
  2012}, pages 5026--5033, 2012.

\bibitem{Uhlenbeck1}
George~E Uhlenbeck and Leonard~S Ornstein.
\newblock On the theory of the brownian motion.
\newblock {\em Physical review}, 1930.

\bibitem{Hasselt1}
Hado van Hasselt.
\newblock Double q-learning.
\newblock In {\em Advances in Neural Information Processing Systems 23: 24th
  Annual Conference on Neural Information Processing Systems 2010. Proceedings
  of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada.},
  pages 2613--2621, 2010.

\bibitem{Harm1}
Harm van Seijen, Hado van Hasselt, Shimon Whiteson, and Marco~A. Wiering.
\newblock A theoretical and empirical analysis of expected sarsa.
\newblock In {\em {IEEE} Symposium on Adaptive Dynamic Programming and
  Reinforcement Learning, {ADPRL} 2009, Nashville, TN, USA, March 31 - April 1,
  2009}, pages 177--184, 2009.

\bibitem{wang1}
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando
  de~Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016}, pages
  1995--2003, 2016.

\bibitem{Watkins1}
Christopher J. C.~H. Watkins and Peter Dayan.
\newblock Q-learning.
\newblock {\em Machine Learning}, 8(3):279--292, May 1992.

\bibitem{Wawr1}
Pawel Wawrzynski.
\newblock Control policy with autocorrelated noise in reinforcement learning
  for robotics.
\newblock {\em International Journal of Machine Learning and Computing},
  5(2):91--95, 2015.

\bibitem{williams1}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine Learning}, 8:229--256, 1992.

\end{thebibliography}
